// Copyright 2023 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

// Code generated by go generate; DO NOT EDIT.

package netdevice

import (
	"context"
	"fmt"
	"reflect"
	"syscall/zx"
	"syscall/zx/zxwait"
	"unsafe"

	"go.fuchsia.dev/fuchsia/src/connectivity/network/netstack/link/fifo"
	"go.fuchsia.dev/fuchsia/src/connectivity/network/netstack/sync"

	"gvisor.dev/gvisor/pkg/tcpip"
	"gvisor.dev/gvisor/pkg/tcpip/stack"
)

type Handler struct {
	TxDepth, RxDepth uint32
	RxFifo, TxFifo   zx.Handle
	rx               Entries
	tx               struct {
		mu struct {
			sync.Mutex
			waiters int
			scratch []uint16

			entries Entries

			// detached signals to incoming writes that the receiver is unable
			// to service them.
			detached bool
		}
		cond sync.Cond
	}
	Stats struct {
		Rx fifo.RxStats
		Tx fifo.TxStats
	}
}

func (h *Handler) TxReceiverLoop(wasSent func(entry *uint16) bool) error {
	scratch := make([]uint16, h.TxDepth)
	for {
		h.tx.mu.Lock()
		detached := h.tx.mu.detached
		h.tx.mu.Unlock()
		if detached {
			return nil
		}

		if err := zxwait.WithRetryContext(context.Background(), func() error {
			status, count := FifoRead(h.TxFifo, scratch)
			if status != zx.ErrOk {
				return &zx.Error{Status: status, Text: "FifoRead(TX)"}
			}
			var notSent uint64
			for i := range scratch[:count] {
				if !wasSent(&scratch[i]) {
					notSent++
				}
			}
			h.Stats.Tx.Drops.IncrementBy(notSent)
			h.Stats.Tx.Reads(count).Increment()
			h.tx.mu.Lock()
			n := h.tx.mu.entries.AddReadied(scratch[:count])
			h.tx.mu.entries.IncrementReadied(uint16(n))
			h.tx.mu.Unlock()
			h.tx.cond.Broadcast()

			if n := uint32(n); count != n {
				return fmt.Errorf("FifoRead(TX): tx_depth invariant violation; observed=%d expected=%d", h.TxDepth-n+count, h.TxDepth)
			}
			return nil
		}, h.TxFifo, zx.SignalFIFOReadable, zx.SignalFIFOPeerClosed); err != nil {
			return err
		}
	}
}

func (h *Handler) RxLoop(process func(entry *uint16)) error {
	scratch := make([]uint16, h.RxDepth)
	for {
		if batchSize := len(scratch) - int(h.rx.InFlight()); batchSize != 0 && h.rx.HaveQueued() {
			n := h.rx.GetQueued(scratch[:batchSize])
			h.rx.IncrementSent(uint16(n))

			status, count := FifoWrite(h.RxFifo, scratch[:n])
			switch status {
			case zx.ErrOk:
				h.Stats.Rx.Writes(count).Increment()
				if n := uint32(n); count != n {
					return fmt.Errorf("FifoWrite(RX): rx_depth invariant violation; observed=%d expected=%d", h.RxDepth-n+count, h.RxDepth)
				}
			default:
				return &zx.Error{Status: status, Text: "FifoWrite(RX)"}
			}
		}

		for h.rx.HaveReadied() {
			entry := h.rx.GetReadied()
			process(entry)
			h.rx.IncrementQueued(1)
		}

		for {
			signals := zx.Signals(zx.SignalFIFOReadable | zx.SignalFIFOPeerClosed)
			if int(h.rx.InFlight()) != len(scratch) && h.rx.HaveQueued() {
				signals |= zx.SignalFIFOWritable
			}
			obs, err := zxwait.WaitContext(context.Background(), h.RxFifo, signals)
			if err != nil {
				return err
			}

			if obs&zx.SignalFIFOPeerClosed != 0 {
				return fmt.Errorf("FifoRead(RX): peer closed")
			}
			if obs&zx.SignalFIFOReadable != 0 {
				switch status, count := FifoRead(h.RxFifo, scratch); status {
				case zx.ErrOk:
					h.Stats.Rx.Reads(count).Increment()
					n := h.rx.AddReadied(scratch[:count])
					h.rx.IncrementReadied(uint16(n))

					if n := uint32(n); count != n {
						return fmt.Errorf("FifoRead(RX): rx_depth invariant violation; observed=%d expected=%d", h.RxDepth-n+count, h.RxDepth)
					}
				default:
					return &zx.Error{Status: status, Text: "FifoRead(RX)"}
				}
				break
			}
			if obs&zx.SignalFIFOWritable != 0 {
				break
			}
		}
	}
}

func (h *Handler) ProcessWrite(pbList stack.PacketBufferList, processor func(*uint16, stack.PacketBufferPtr)) (int, tcpip.Error) {
	pkts := pbList.AsSlice()
	i := 0

	for i < len(pkts) {
		h.tx.mu.Lock()
		for {
			if h.tx.mu.detached {
				h.tx.mu.Unlock()
				return i, &tcpip.ErrClosedForSend{}
			}

			if h.tx.mu.entries.HaveReadied() {
				break
			}

			h.tx.mu.waiters++
			h.tx.cond.Wait()
			h.tx.mu.waiters--
		}

		// Queue as many remaining packets as possible; if we run out of space,
		// we'll return to the waiting state in the outer loop.
		for ; i < len(pkts) && h.tx.mu.entries.HaveReadied(); i++ {
			entry := h.tx.mu.entries.GetReadied()
			processor(entry, pkts[i])
			h.tx.mu.entries.IncrementQueued(1)
		}

		batch := h.tx.mu.scratch[:len(h.tx.mu.scratch)-int(h.tx.mu.entries.InFlight())]
		n := h.tx.mu.entries.GetQueued(batch)
		h.tx.mu.entries.IncrementSent(uint16(n))
		// We must write to the FIFO under lock because `batch` is aliased from
		// `h.tx.mu.scratch`.
		status, count := FifoWrite(h.TxFifo, batch[:n])
		h.tx.mu.Unlock()

		switch status {
		case zx.ErrOk:
			if n := uint32(n); count != n {
				panic(fmt.Sprintf("FifoWrite(TX): tx_depth invariant violation; observed=%d expected=%d", h.TxDepth-n+count, h.TxDepth))
			}
			h.Stats.Tx.Writes(count).Increment()
		case zx.ErrPeerClosed:
			h.DetachTx()
			return i, &tcpip.ErrClosedForSend{}
		case zx.ErrBadHandle:
			// We may have detached then closed the FIFO since we last unlocked before
			// writing to the FIFO.
			h.tx.mu.Lock()
			detached := h.tx.mu.detached
			h.tx.mu.Unlock()
			if detached {
				return i, &tcpip.ErrClosedForSend{}
			}
			fallthrough
		default:
			panic(fmt.Sprintf("FifoWrite(TX): (%v, %d)", status, count))
		}
	}

	return i, nil
}

func (h *Handler) DetachTx() {
	h.tx.mu.Lock()
	h.tx.mu.detached = true
	h.tx.mu.Unlock()
	h.tx.cond.Broadcast()
}

func (h *Handler) InitRx(capacity uint16) uint16 {
	return h.rx.Init(capacity)
}

func (h *Handler) InitTx(capacity uint16) uint16 {
	h.tx.cond.L = &h.tx.mu.Mutex
	h.tx.mu.Lock()
	h.tx.mu.scratch = make([]uint16, h.TxDepth)
	h.tx.mu.Unlock()
	return h.tx.mu.entries.Init(capacity)
}

func (h *Handler) PushInitialRx(entry uint16) {
	h.rx.storage[h.rx.readied] = entry
	h.rx.IncrementReadied(1)
	h.rx.IncrementQueued(1)
}

func (h *Handler) PushInitialTx(entry uint16) {
	h.tx.mu.entries.storage[h.tx.mu.entries.readied] = entry
	h.tx.mu.entries.IncrementReadied(1)
}

func FifoWrite(handle zx.Handle, b []uint16) (zx.Status, uint32) {
	var actual uint
	var _x uint16
	data := unsafe.Pointer((*reflect.SliceHeader)(unsafe.Pointer(&b)).Data)

	// TODO(https://fxbug.dev/32098): We're assuming that writing to the FIFO
	// here is a sufficient memory barrier for the other end to access the data.
	// That is currently true but not really guaranteed by the API.

	status := zx.Sys_fifo_write(handle, uint(unsafe.Sizeof(_x)), data, uint(len(b)), &actual)
	return status, uint32(actual)
}

func FifoRead(handle zx.Handle, b []uint16) (zx.Status, uint32) {
	var actual uint
	var _x uint16
	data := unsafe.Pointer((*reflect.SliceHeader)(unsafe.Pointer(&b)).Data)
	status := zx.Sys_fifo_read(handle, uint(unsafe.Sizeof(_x)), data, uint(len(b)), &actual)
	return status, uint32(actual)
}
