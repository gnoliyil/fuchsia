// Copyright 2022 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

use {
    crate::address::GuestPhysicalAddress, crate::hypervisor::Hypervisor, fuchsia_zircon as zx,
    futures::channel::oneshot, std::sync::Arc, std::thread::JoinHandle,
};

// [VcpuState] manages the internal lifecycle of a [Vcpu].
#[derive(Debug)]
enum VcpuState<H: Hypervisor> {
    Created {
        guest: Arc<H::GuestHandle>,
        entry: GuestPhysicalAddress,
        boot_ptr: u64,
    },
    Starting,
    Running {
        control_handle: H::VcpuControlHandle,
    },
    Failed {
        #[allow(dead_code)]
        status: zx::Status,
    },
}

pub struct Vcpu<H: Hypervisor> {
    hypervisor: H,
    id: u32,
    state: VcpuState<H>,
    join_handle: Option<JoinHandle<Result<(), zx::Status>>>,
}

impl<H: Hypervisor> std::ops::Drop for Vcpu<H> {
    fn drop(&mut self) {
        let hypervisor = self.hypervisor.clone();
        if let VcpuState::Running { control_handle, .. } = &self.state {
            // It's possible that the thread has already shutdown, meaning we won't be able
            // to kick the thread, so we do not care about any failures here.
            let _ = hypervisor.vcpu_kick(control_handle);
        }
        if let Some(join_handle) = self.join_handle.take() {
            if let Err(e) = join_handle.join() {
                std::panic::resume_unwind(e);
            }
        }
    }
}

impl<H: Hypervisor> Vcpu<H> {
    pub fn new(
        hypervisor: H,
        guest: Arc<H::GuestHandle>,
        id: u32,
        entry: GuestPhysicalAddress,
        boot_ptr: u64,
    ) -> Self {
        Self {
            hypervisor,
            id,
            state: VcpuState::Created { guest, entry, boot_ptr },
            join_handle: None,
        }
    }

    pub async fn start(&mut self) -> zx::Status {
        let (result_sender, result_receiver) =
            oneshot::channel::<Result<H::VcpuControlHandle, zx::Status>>();
        let VcpuState::Created { guest, entry, boot_ptr} = std::mem::replace(&mut self.state, VcpuState::Starting) else {
            return zx::Status::BAD_STATE;
        };
        let id = self.id;
        let hypervisor = self.hypervisor.clone();
        let guest = guest.clone();

        assert!(self.join_handle.is_none());
        let join_handle = std::thread::Builder::new()
            .name(format!("vcpu-{}", id))
            .spawn(move || Self::vcpu_main(hypervisor, guest, id, entry, boot_ptr, result_sender))
            .expect("Failed to spawn vcpu thread");
        let status =
            result_receiver.await.unwrap_or_else(|oneshot::Canceled| Err(zx::Status::CANCELED));
        match status {
            Ok(control_handle) => {
                self.state = VcpuState::Running { control_handle };
                self.join_handle = Some(join_handle);
                zx::Status::OK
            }
            Err(status) => {
                self.state = VcpuState::Failed { status };
                if let Err(e) = join_handle.join() {
                    std::panic::resume_unwind(e);
                }
                status
            }
        }
    }

    // The primary entry point for the vcpu thread.
    //
    // This will create the [Hypervisor::VcpuHandle] and do all the necessary initialization. Once
    // initialization has completed the result will be sent over `result_sender`. In the case that
    // initialization fails this method _must_ return because the caller will join the thread before
    // continuing.
    //
    // Once the vcpu has initialized, it will be start execution using [Hypervisor::vcpu_enter] and
    // handle any IO or vcpu control packets from the hypervisor. This will continue until either
    //
    //     1. An unhandled or unsupported packet is generated by the hypervisor.
    //     2. [Hypervisor::vcpu_enter] return [zx::Status::CANCELLED], signalling a graceful
    //        termination of the vcpu.
    fn vcpu_main(
        hypervisor: H,
        guest: Arc<H::GuestHandle>,
        id: u32,
        entry: GuestPhysicalAddress,
        boot_ptr: u64,
        result_sender: oneshot::Sender<Result<H::VcpuControlHandle, zx::Status>>,
    ) -> Result<(), zx::Status> {
        // Create vcpu object.
        let (vcpu, vcpu_controller) = match hypervisor.vcpu_create(guest.as_ref(), entry) {
            Ok(result) => result,
            Err(status) => {
                tracing::warn!(%status, vcpu_id = id, "Failed to create vcpu");
                result_sender.send(Err(status)).unwrap();
                return Err(status);
            }
        };

        // Write initial vcpu state.
        //
        // The instruction pointer is initialized by `vcpu_create` but the other registers can be
        // written here.
        if let Err(status) = Self::write_initial_vcpu_state(&hypervisor, &vcpu, boot_ptr) {
            tracing::warn!(%status, vcpu_id = id, "Failed to write vcpu state");
            result_sender.send(Err(status)).unwrap();
            return Err(status);
        }

        // Initialization completed, report this to the main thread.
        //
        // We `unwrap` here because `send` will only fail if the receiving endpoint of the channel
        // has already closed. We ensure in [Vcpu::start] that this does not happen and it's a
        // logic error otherwise.
        result_sender.send(Ok(vcpu_controller)).unwrap();

        loop {
            // Resume vcpu execution with the hypervisor. This will transfer execution to the guest
            // until a vm-exit occurs that needs intervetion from the vmm.
            let packet = match hypervisor.vcpu_enter(&vcpu) {
                Ok(packet) => packet,
                Err(zx::Status::CANCELED) => {
                    tracing::info!(vcpu_id = id, "Stopping vcpu");
                    return Ok(());
                }
                Err(e) => {
                    tracing::error!(
                        %e, vcpu_id = id, "Fatal error attempting to enter vcpu; Shutting down VM.",
                    );
                    return Err(e);
                }
            };

            // TODO(https://fxbug.dev/102872): Read and dispatch vm-exit packets.
            let _ = packet;
        }
    }

    #[cfg(target_arch = "x86_64")]
    fn write_initial_vcpu_state(
        hypervisor: &H,
        vcpu: &H::VcpuHandle,
        boot_ptr: u64,
    ) -> Result<(), zx::Status> {
        let mut vcpu_state: zx::sys::zx_vcpu_state_t = Default::default();
        vcpu_state.rsi = boot_ptr.try_into().unwrap();
        hypervisor.vcpu_write_state(vcpu, &vcpu_state)
    }

    #[cfg(target_arch = "aarch64")]
    fn write_initial_vcpu_state(
        hypervisor: &H,
        vcpu: &H::VcpuHandle,
        boot_ptr: u64,
    ) -> Result<(), zx::Status> {
        let mut vcpu_state: zx::sys::zx_vcpu_state_t = Default::default();
        vcpu_state.x[0] = boot_ptr.try_into().unwrap();
        hypervisor.vcpu_write_state(vcpu, &vcpu_state)
    }
}

#[cfg(test)]
mod tests {
    use {
        super::*,
        crate::hypervisor::testing::{MockBehavior, MockHypervisor},
    };

    impl<H: Hypervisor> Vcpu<H> {
        /// Read the name of the vcpu thread.
        ///
        /// Returns [None] if there is no thread, or if the thread is unnamed.
        #[cfg(test)]
        pub fn thread_name(&self) -> Option<&str> {
            self.join_handle.as_ref().and_then(|h| h.thread().name())
        }

        /// Returns the [JoinHandle] for the underlying thread for the [Vcpu].
        ///
        /// This will return [None] if the thread has not yet started, or if the [JoinHandle] has already
        /// been taken.
        ///
        /// Tests that take the [JoinHandle] _must_ join on it, the test runner can fail in various ways
        /// if the test exits without joining all the created threads.
        #[cfg(test)]
        pub fn take_join_handle(&mut self) -> Option<JoinHandle<Result<(), zx::Status>>> {
            self.join_handle.take()
        }
    }

    #[fuchsia::test]
    async fn test_vcpu_create_fails() {
        let hypervisor = MockHypervisor::new();
        hypervisor.on_vcpu_create(MockBehavior::ReturnError(zx::Status::ACCESS_DENIED));

        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);
        let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), 0, 0, 0);

        assert_eq!(vcpu.start().await, zx::Status::ACCESS_DENIED);
        assert_eq!(guest.vcpus().len(), 0);
    }

    #[fuchsia::test]
    async fn test_vcpu_write_state_fails() {
        let hypervisor = MockHypervisor::new();
        hypervisor.on_vcpu_write_state(MockBehavior::ReturnError(zx::Status::BAD_STATE));

        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);
        let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), 0, 0, 0);

        assert_eq!(vcpu.start().await, zx::Status::BAD_STATE);
        // Expect 1 vcpu from the guest perspective because we do expect to have successfully
        // created the guest using the hypervisor.
        assert_eq!(guest.vcpus().len(), 1);
    }

    #[fuchsia::test]
    async fn test_create() {
        let hypervisor = MockHypervisor::new();
        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);
        let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), 0, 0, 0);
        assert_eq!(vcpu.start().await, zx::Status::OK);
        assert_eq!(guest.vcpus().len(), 1);
    }

    #[fuchsia::test]
    async fn test_vcpu_shutdown() {
        let hypervisor = MockHypervisor::new();
        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);
        let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), 0, 0, 0);
        assert_eq!(vcpu.start().await, zx::Status::OK);

        // Expect the vcpu was created successfully.
        let vcpus = guest.vcpus();
        assert_eq!(vcpus.len(), 1);

        // When zx_vcpu_enter returns [zx::Status::CANCELLED], this indicates a graceful shutdown. As a result we
        // expect the backing vcpu thread to exit and return zx::Status::OK in this situation.
        let join_handle = vcpu.take_join_handle().expect("Failed to obtain vcpu JoinHandle");
        vcpus[0].send_status(zx::Status::CANCELED).unwrap();
        assert_eq!(Ok(()), join_handle.join().expect("Failed to join vcpu thread."));
    }

    #[fuchsia::test]
    async fn test_vcpu_threads_are_named_correctly() {
        let hypervisor = MockHypervisor::new();
        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);

        // Start several vcpus.
        let mut vcpus = Vec::new();
        for vcpu_id in 0..4 {
            let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), vcpu_id, 0, 0);
            assert_eq!(vcpu.start().await, zx::Status::OK);
            vcpus.push(vcpu);
        }

        let vcpu_controllers = guest.vcpus();
        assert_eq!(vcpu_controllers.len(), vcpus.len());
        for vcpu_id in 0..4 {
            let thread_name =
                vcpus[vcpu_id].thread_name().expect("Unable to read vcpu thread name");
            assert_eq!(thread_name, format!("vcpu-{}", vcpu_id));
        }
    }
}
