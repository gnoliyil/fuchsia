// Copyright 2022 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

use {
    crate::address::GuestPhysicalAddress, crate::device_manager::DeviceManager,
    crate::hypervisor::Hypervisor, fuchsia_zircon as zx, futures::channel::oneshot, std::sync::Arc,
    std::thread::JoinHandle, vm_device::bus::PioAddress,
};

// [VcpuState] manages the internal lifecycle of a [Vcpu].
enum VcpuState<H: Hypervisor> {
    Created {
        guest: Arc<H::GuestHandle>,
        entry: GuestPhysicalAddress,
        boot_ptr: u64,
        devices: DeviceManager<H>,
    },
    Starting,
    Running {
        control_handle: H::VcpuControlHandle,
    },
    Failed {
        #[allow(dead_code)]
        status: zx::Status,
    },
}

pub struct Vcpu<H: Hypervisor> {
    hypervisor: H,
    id: u32,
    state: VcpuState<H>,
    join_handle: Option<JoinHandle<Result<(), zx::Status>>>,
}

impl<H: Hypervisor> std::ops::Drop for Vcpu<H> {
    fn drop(&mut self) {
        let hypervisor = self.hypervisor.clone();
        if let VcpuState::Running { control_handle, .. } = &self.state {
            // It's possible that the thread has already shutdown, meaning we won't be able
            // to kick the thread, so we do not care about any failures here.
            let _ = hypervisor.vcpu_kick(control_handle);
        }
        if let Some(join_handle) = self.join_handle.take() {
            if let Err(e) = join_handle.join() {
                std::panic::resume_unwind(e);
            }
        }
    }
}

impl<H: Hypervisor> Vcpu<H> {
    pub fn new(
        hypervisor: H,
        guest: Arc<H::GuestHandle>,
        devices: DeviceManager<H>,
        id: u32,
        entry: GuestPhysicalAddress,
        boot_ptr: u64,
    ) -> Self {
        Self {
            hypervisor,
            id,
            state: VcpuState::Created { guest, entry, boot_ptr, devices },
            join_handle: None,
        }
    }

    pub async fn start(&mut self) -> zx::Status {
        let (result_sender, result_receiver) =
            oneshot::channel::<Result<H::VcpuControlHandle, zx::Status>>();
        let VcpuState::Created { guest, entry, boot_ptr, devices } = std::mem::replace(&mut self.state, VcpuState::Starting) else {
            return zx::Status::BAD_STATE;
        };
        let id = self.id;
        let hypervisor = self.hypervisor.clone();
        let guest = guest.clone();

        assert!(self.join_handle.is_none());
        let join_handle = std::thread::Builder::new()
            .name(format!("vcpu-{}", id))
            .spawn(move || {
                Self::vcpu_main(hypervisor, guest, devices, id, entry, boot_ptr, result_sender)
            })
            .expect("Failed to spawn vcpu thread");
        let status =
            result_receiver.await.unwrap_or_else(|oneshot::Canceled| Err(zx::Status::CANCELED));
        match status {
            Ok(control_handle) => {
                self.state = VcpuState::Running { control_handle };
                self.join_handle = Some(join_handle);
                zx::Status::OK
            }
            Err(status) => {
                self.state = VcpuState::Failed { status };
                if let Err(e) = join_handle.join() {
                    std::panic::resume_unwind(e);
                }
                status
            }
        }
    }

    // The primary entry point for the vcpu thread.
    //
    // This will create the [Hypervisor::VcpuHandle] and do all the necessary initialization. Once
    // initialization has completed the result will be sent over `result_sender`. In the case that
    // initialization fails this method _must_ return because the caller will join the thread before
    // continuing.
    //
    // Once the vcpu has initialized, it will be start execution using [Hypervisor::vcpu_enter] and
    // handle any IO or vcpu control packets from the hypervisor. This will continue until either
    //
    //     1. An unhandled or unsupported packet is generated by the hypervisor.
    //     2. [Hypervisor::vcpu_enter] return [zx::Status::CANCELED], signalling a graceful
    //        termination of the vcpu.
    fn vcpu_main(
        hypervisor: H,
        guest: Arc<H::GuestHandle>,
        devices: DeviceManager<H>,
        id: u32,
        entry: GuestPhysicalAddress,
        boot_ptr: u64,
        result_sender: oneshot::Sender<Result<H::VcpuControlHandle, zx::Status>>,
    ) -> Result<(), zx::Status> {
        // Create vcpu object.
        let (vcpu, vcpu_controller) = match hypervisor.vcpu_create(guest.as_ref(), entry) {
            Ok(result) => result,
            Err(status) => {
                tracing::warn!(%status, vcpu_id = id, "Failed to create vcpu");
                result_sender.send(Err(status)).unwrap();
                return Err(status);
            }
        };

        // Write initial vcpu state.
        //
        // The instruction pointer is initialized by `vcpu_create` but the other registers can be
        // written here.
        if let Err(status) = Self::write_initial_vcpu_state(&hypervisor, &vcpu, boot_ptr) {
            tracing::warn!(%status, vcpu_id = id, "Failed to write vcpu state");
            result_sender.send(Err(status)).unwrap();
            return Err(status);
        }

        // Initialization completed, report this to the main thread.
        //
        // We `unwrap` here because `send` will only fail if the receiving endpoint of the channel
        // has already closed. We ensure in [Vcpu::start] that this does not happen and it's a
        // logic error otherwise.
        result_sender.send(Ok(vcpu_controller)).unwrap();

        loop {
            // Resume vcpu execution with the hypervisor. This will transfer execution to the guest
            // until a vm-exit occurs that needs intervetion from the vmm.
            let packet = match hypervisor.vcpu_enter(&vcpu) {
                Ok(packet) => packet,
                Err(zx::Status::CANCELED) => {
                    tracing::info!(vcpu_id = id, "Stopping vcpu");
                    return Ok(());
                }
                Err(e) => {
                    tracing::error!(
                        %e, vcpu_id = id, "Fatal error attempting to enter vcpu; Shutting down VM.",
                    );
                    return Err(e);
                }
            };

            match handle_packet(&devices, packet) {
                // If the packet handling succeeded, we can resume execution of the vcpu.
                Ok(()) => continue,
                // `CANCELED` means a graceful shutdown has been requested so we will stop running the
                // vcpu.
                Err(zx::Status::CANCELED) => return Ok(()),
                // Propagate other errors.
                Err(status) => return Err(status),
            }
        }
    }

    #[cfg(target_arch = "x86_64")]
    fn write_initial_vcpu_state(
        hypervisor: &H,
        vcpu: &H::VcpuHandle,
        boot_ptr: u64,
    ) -> Result<(), zx::Status> {
        let mut vcpu_state: zx::sys::zx_vcpu_state_t = Default::default();
        vcpu_state.rsi = boot_ptr.try_into().unwrap();
        hypervisor.vcpu_write_state(vcpu, &vcpu_state)
    }

    #[cfg(target_arch = "aarch64")]
    fn write_initial_vcpu_state(
        hypervisor: &H,
        vcpu: &H::VcpuHandle,
        boot_ptr: u64,
    ) -> Result<(), zx::Status> {
        let mut vcpu_state: zx::sys::zx_vcpu_state_t = Default::default();
        vcpu_state.x[0] = boot_ptr.try_into().unwrap();
        hypervisor.vcpu_write_state(vcpu, &vcpu_state)
    }
}

fn handle_packet<H: Hypervisor>(
    devices: &DeviceManager<H>,
    packet: zx::Packet,
) -> Result<(), zx::Status> {
    match packet.contents() {
        zx::PacketContents::GuestIo(pio) => handle_pio_packet(devices, pio),
        zx::PacketContents::GuestMem(mmio) => handle_mmio_packet(devices, mmio),
        zx::PacketContents::GuestVcpu(vcpu) => handle_vcpu_packet(devices, vcpu),
        unhandled_packet => {
            tracing::error!(?unhandled_packet, "Received unsupported packet");
            Err(zx::Status::NOT_SUPPORTED)
        }
    }
}

fn handle_pio_packet<H: Hypervisor>(
    devices: &DeviceManager<H>,
    pio: zx::GuestIoPacket,
) -> Result<(), zx::Status> {
    match pio.access_type() {
        zx::AccessType::Write => match pio.data() {
            Some(zx::PortData::Data8(v)) => {
                devices.pio_write(PioAddress(pio.port()), &v.to_ne_bytes())
            }
            Some(zx::PortData::Data16(v)) => {
                devices.pio_write(PioAddress(pio.port()), &v.to_ne_bytes())
            }
            Some(zx::PortData::Data32(v)) => {
                devices.pio_write(PioAddress(pio.port()), &v.to_ne_bytes())
            }
            None => {
                tracing::error!("Hypervisor generated a pio write packet without associated data");
                return Err(zx::Status::INTERNAL);
            }
        },
        zx::AccessType::Read => unimplemented!(),
    }
}

fn handle_mmio_packet<H: Hypervisor>(
    _devices: &DeviceManager<H>,
    _mmio: zx::GuestMemPacket,
) -> Result<(), zx::Status> {
    // TODO(https://fxbug.dev/119424): For x64 we need to decode the instructions in the
    // `mmio` packet to determine the parameters of the mmio access.
    unimplemented!();
}

fn handle_vcpu_packet<H: Hypervisor>(
    _devices: &DeviceManager<H>,
    _vcpu: zx::GuestVcpuPacket,
) -> Result<(), zx::Status> {
    // TODO(https://fxbug.dev/102872): Add support for vcpu packets.
    unimplemented!();
}

#[cfg(test)]
mod tests {
    use {
        super::*,
        crate::hw::MemoryDevice,
        crate::hypervisor::testing::{MockBehavior, MockGuest, MockHypervisor, MockVcpuController},
        std::sync::{Arc, Mutex},
        vm_device::bus::PioRange,
    };

    impl<H: Hypervisor> Vcpu<H> {
        /// Read the name of the vcpu thread.
        ///
        /// Returns [None] if there is no thread, or if the thread is unnamed.
        #[cfg(test)]
        pub fn thread_name(&self) -> Option<&str> {
            self.join_handle.as_ref().and_then(|h| h.thread().name())
        }

        /// Returns the [JoinHandle] for the underlying thread for the [Vcpu].
        ///
        /// This will return [None] if the thread has not yet started, or if the [JoinHandle] has already
        /// been taken.
        ///
        /// Tests that take the [JoinHandle] _must_ join on it, the test runner can fail in various ways
        /// if the test exits without joining all the created threads.
        #[cfg(test)]
        pub fn take_join_handle(&mut self) -> Option<JoinHandle<Result<(), zx::Status>>> {
            self.join_handle.take()
        }

        /// Returns the [Hypervisor::ControlHandle] for the running vcpu.
        ///
        /// Returns a reference to the control handle if the vcpu is running. If the vcpu is not running
        /// (and thus has no control handle) [None] is returned.
        pub fn control_handle(&self) -> Option<&H::VcpuControlHandle> {
            if let VcpuState::Running { control_handle } = &self.state {
                Some(control_handle)
            } else {
                None
            }
        }
    }

    struct TestFixture {
        hypervisor: MockHypervisor,
        guest: Arc<MockGuest>,
        devices: DeviceManager<MockHypervisor>,
    }

    impl TestFixture {
        pub fn new() -> Self {
            let hypervisor = MockHypervisor::new();
            let (guest, _address_space) = hypervisor.guest_create().unwrap();
            let guest = Arc::new(guest);
            let devices = DeviceManager::new(hypervisor.clone(), guest.clone());
            Self { hypervisor, guest, devices }
        }

        pub fn create_vcpu(
            &self,
            id: u32,
            entry: GuestPhysicalAddress,
            boot_ptr: u64,
        ) -> Vcpu<MockHypervisor> {
            Vcpu::new(
                self.hypervisor.clone(),
                self.guest.clone(),
                self.devices.clone(),
                id,
                entry,
                boot_ptr,
            )
        }

        pub async fn create_and_start_vcpu(
            &self,
            id: u32,
            entry: GuestPhysicalAddress,
            boot_ptr: u64,
        ) -> (Vcpu<MockHypervisor>, MockVcpuController) {
            let mut vcpu = self.create_vcpu(id, entry, boot_ptr);
            assert_eq!(vcpu.start().await, zx::Status::OK);

            // Unwrap here because this should only be None if the vcpu is not
            // running.
            let control_handle = vcpu.control_handle().unwrap().clone();
            (vcpu, control_handle)
        }
    }

    #[fuchsia::test]
    async fn test_vcpu_create_fails() {
        let hypervisor = MockHypervisor::new();
        hypervisor.on_vcpu_create(MockBehavior::ReturnError(zx::Status::ACCESS_DENIED));

        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);
        let devices = DeviceManager::new(hypervisor.clone(), guest.clone());
        let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), devices, 0, 0, 0);

        assert_eq!(vcpu.start().await, zx::Status::ACCESS_DENIED);
        assert_eq!(guest.vcpus().len(), 0);
    }

    #[fuchsia::test]
    async fn test_vcpu_write_state_fails() {
        let hypervisor = MockHypervisor::new();
        hypervisor.on_vcpu_write_state(MockBehavior::ReturnError(zx::Status::BAD_STATE));

        let (guest, _address_space) = hypervisor.guest_create().unwrap();
        let guest = Arc::new(guest);
        let devices = DeviceManager::new(hypervisor.clone(), guest.clone());
        let mut vcpu = Vcpu::new(hypervisor.clone(), guest.clone(), devices, 0, 0, 0);

        assert_eq!(vcpu.start().await, zx::Status::BAD_STATE);
        // Expect 1 vcpu from the guest perspective because we do expect to have successfully
        // created the guest using the hypervisor.
        assert_eq!(guest.vcpus().len(), 1);
    }

    #[fuchsia::test]
    async fn test_create() {
        let test = TestFixture::new();

        let mut vcpu = test.create_vcpu(0, 0, 0);

        assert_eq!(vcpu.start().await, zx::Status::OK);
        assert_eq!(test.guest.vcpus().len(), 1);
    }

    #[fuchsia::test]
    async fn test_vcpu_shutdown() {
        let test = TestFixture::new();

        // When zx_vcpu_enter returns [zx::Status::CANCELED], this indicates a graceful shutdown. As a result we
        // expect the backing vcpu thread to exit and return zx::Status::OK in this situation.
        let (mut vcpu, vcpu_controller) = test.create_and_start_vcpu(0, 0, 0).await;
        let join_handle = vcpu.take_join_handle().expect("Failed to obtain vcpu JoinHandle");

        vcpu_controller.send_status(zx::Status::CANCELED).unwrap();
        assert_eq!(Ok(()), join_handle.join().expect("Failed to join vcpu thread."));
    }

    #[fuchsia::test]
    async fn test_vcpu_threads_are_named_correctly() {
        let test = TestFixture::new();

        // Start several vcpus.
        let mut vcpus = Vec::new();
        for vcpu_id in 0..4 {
            vcpus.push(test.create_and_start_vcpu(vcpu_id, 0, 0).await);
        }

        for vcpu_id in 0..4 {
            let thread_name =
                vcpus[vcpu_id].0.thread_name().expect("Unable to read vcpu thread name");
            assert_eq!(thread_name, format!("vcpu-{}", vcpu_id));
        }
    }

    #[fuchsia::test]
    async fn test_vcpu_pio_write() {
        let test = TestFixture::new();

        // Create a test device on the PIO bus.
        let ram_device = Arc::new(Mutex::new(MemoryDevice::ram_bytes(100)));
        test.devices
            .register_pio(PioRange::new(PioAddress(0x1000), 100).unwrap(), ram_device.clone())
            .unwrap();

        // Create and start a vcpu.
        let (_vcpu, vcpu_controller) = test.create_and_start_vcpu(0, 0, 0).await;

        // Simulate an IO trap by sending a port packet to the vcpu.
        let io_packet = zx::GuestIoPacket::from_raw(zx::sys::zx_packet_guest_io_t {
            port: 0x1000,
            access_size: 1,
            input: false,
            data: [0xab, 0xcd, 0xef, 0x12],
        });
        vcpu_controller
            .send_packet(zx::Packet::from_guest_io_packet(0, zx::sys::ZX_OK, io_packet))
            .unwrap();

        // Once the vcpu has called vcpu_enter for the second time we know it should have handled
        // the trap.
        vcpu_controller.entry_counter().wait_for_count(2);
        // Expect the first byte to be written but not the others.
        assert_eq!(ram_device.lock().unwrap().as_slice()[0], 0xab);
        assert_eq!(ram_device.lock().unwrap().as_slice()[1], 0x00);
    }

    #[fuchsia::test]
    async fn test_vcpu_pio_write_unmapped_address() {
        let test = TestFixture::new();

        // Create and start a vcpu.
        let (mut vcpu, vcpu_controller) = test.create_and_start_vcpu(0, 0, 0).await;

        // Send an IO trap to an address with no device mapping.
        let io_packet = zx::GuestIoPacket::from_raw(zx::sys::zx_packet_guest_io_t {
            port: 0x1000,
            access_size: 1,
            input: false,
            data: [0, 0, 0, 0],
        });
        vcpu_controller
            .send_packet(zx::Packet::from_guest_io_packet(0, zx::sys::ZX_OK, io_packet))
            .unwrap();

        // Expect the vcpu to exit in response to the unhandled trap.
        let join_handle = vcpu.take_join_handle().expect("Failed to obtain vcpu JoinHandle");
        assert_eq!(join_handle.join().unwrap(), Err(zx::Status::NOT_FOUND));
        assert_eq!(vcpu_controller.entry_counter().get_count(), 1);
    }
}
