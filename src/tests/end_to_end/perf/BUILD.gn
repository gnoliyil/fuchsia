# Copyright 2019 The Fuchsia Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

import("//build/dart/dart_library.gni")
import("//build/dart/test.gni")
import("//build/testing/environments.gni")
import("//build/testing/host_test_data.gni")
import("//build/testing/perf/test.gni")

# This directory contains Dart wrappers for running performance tests
# that are defined elsewhere in the Fuchsia tree.
#
# The test wrappers in this directory are split into multiple test
# executables (dart_test targets here) because the Fuchsia Infra tools
# for retrying tests and tracking flakes operate on the granularity of
# test executables.

test_list = [
  # keep-sorted start
  "flatland_benchmarks_test",
  "input_latency_benchmarks_test",

  # keep-sorted end
]

if (!is_asan && host_os == "linux") {
  test_list += [ "storage_starnix_benchmarks_test" ]
}

foreach(target_name, test_list) {
  dart_test(target_name) {
    sources = [
      "$target_name.dart",
      "helpers.dart",
      "summarize.dart",
    ]

    deps = [
      "//sdk/testing/sl4f/client",
      "//third_party/dart-pkg/pub/args",
      "//third_party/dart-pkg/pub/logging",
      "//third_party/dart-pkg/pub/meta",
      "//third_party/dart-pkg/pub/test",
      "//third_party/dart-pkg/pub/tuple",
    ]

    non_dart_deps = [ ":runtime_deps($host_toolchain)" ]

    environments = [
      nuc_env,
      vim3_env,
    ]
  }
}

dart_test("summarize_perf_results_test") {
  sources = [
    "summarize.dart",
    "summarize_test.dart",
  ]

  deps = [
    "//sdk/testing/sl4f/client",
    "//third_party/dart-pkg/pub/args",
    "//third_party/dart-pkg/pub/logging",
    "//third_party/dart-pkg/pub/meta",
    "//third_party/dart-pkg/pub/test",
    "//third_party/dart-pkg/pub/tuple",
  ]

  non_dart_deps = [ ":runtime_deps($host_toolchain)" ]
}
test_list += [ "summarize_perf_results_test" ]

# Split the following tests into shards so that each shard fits
# within the default time limit.
shards = [
  "0",
  "1",
  "2",
  "3",
  "4",
  "5",
  "6",
  "7",
  "8",
  "9",
  "10",
]
total_shards = "11"
shard_test_list = [ "netstack_iperf_test" ]

foreach(test, shard_test_list) {
  foreach(shard, shards) {
    target_name = test + "_shard_" + shard
    dart_test(target_name) {
      sources = [
        test + ".dart",
        "helpers.dart",
        "summarize.dart",
      ]

      args = [
        "--total-shards=" + total_shards,
        "--shard-index=" + shard,
      ]

      deps = [
        "//sdk/testing/sl4f/client",
        "//third_party/dart-pkg/pub/args",
        "//third_party/dart-pkg/pub/logging",
        "//third_party/dart-pkg/pub/meta",
        "//third_party/dart-pkg/pub/test",
        "//third_party/dart-pkg/pub/tuple",
      ]

      non_dart_deps = [ ":runtime_deps($host_toolchain)" ]

      environments = [
        nuc_env,
        vim3_env,
      ]
    }
    test_list += [ target_name ]
  }
}

_tests = [
  {
    name = "flatland_benchmark"
    expected_metric_filenames = [ "fuchsia.flatland_latency.txt" ]
  },
  {
    name = "input_latency_benchmark"
    expected_metric_filenames = [ "fuchsia.input_latency.simplest_app.txt" ]
    transport = "sl4f"
  },
  {
    name = "perf_publish_example"
    expected_metric_filenames = [ "fuchsia.example.txt" ]
    libraries = [ "//src/performance/lib/perf_publish" ]
  },
  {
    name = "perftest_trace_events_test"
    expected_metric_filenames = []
    libraries = [ "//src/performance/lib/trace_processing" ]
  },
  {
    name = "tracing_microbenchmarks_test"
    expected_metric_filenames = [
      "fuchsia.microbenchmarks.tracing.txt",
      "fuchsia.microbenchmarks.tracing_categories_disabled.txt",
      "fuchsia.trace_records.rust.tracing_categories_disabled.txt",
      "fuchsia.trace_records.rust.tracing_disabled.txt",
    ]
    libraries = [
      "//src/performance/lib/perf_publish",
      "//src/performance/lib/py_test_utils:perf_test_utils",
      "//src/performance/lib/trace_processing",
    ]
    target_deps = [
      "//src/lib/trace/rust/bench",
      "//src/tests/microbenchmarks:fuchsia_microbenchmarks",
    ]
  },
]

_python_benchmarks = []
foreach(test, _tests) {
  _name = test.name
  _test_name = "py_${_name}"
  if (is_host) {
    python_perf_test(_test_name) {
      main_source = "test/${_name}.py"
      forward_variables_from(test,
                             [
                               "transport",
                               "target_deps",
                             ])
      expected_metric_names_filepaths = []
      foreach(filename, test.expected_metric_filenames) {
        expected_metric_names_filepaths +=
            [ "//src/tests/end_to_end/perf/expected_metric_names/${filename}" ]
      }
      libraries =
          [ "//src/testing/end_to_end/mobly_base_tests:fuchsia_base_test" ]
      if (defined(test.libraries)) {
        libraries += test.libraries
      }
    }
  }
  _python_benchmarks += [ ":${_test_name}($host_toolchain)" ]
}

if (is_host) {
  metric_files = [
    # keep-sorted start
    "fuchsia.flatland_latency.txt",
    "fuchsia.input_latency.simplest_app.txt",
    "fuchsia.netstack.iperf_benchmarks.ethernet_tcp_recv.txt",
    "fuchsia.netstack.iperf_benchmarks.ethernet_tcp_send.txt",
    "fuchsia.netstack.iperf_benchmarks.ethernet_udp_recv.txt",
    "fuchsia.netstack.iperf_benchmarks.ethernet_udp_send.txt",
    "fuchsia.netstack.iperf_benchmarks.localhost_tcp.txt",
    "fuchsia.netstack.iperf_benchmarks.localhost_udp.txt",
    "fuchsia.storage.starnix_data.txt",
    "fuchsia.storage.starnix_tmp.txt",

    # keep-sorted end
  ]

  host_test_data("runtime_deps") {
    sources = [
      "$root_out_dir/catapult_converter",
      "$root_out_dir/trace2json",
    ]
    foreach(filename, metric_files) {
      sources += [ "expected_metric_names/$filename" ]
    }
    outputs = [ "$target_gen_dir/runtime_deps/{{source_file_part}}" ]

    deps = [
      "//src/performance/trace2json:bin",
      "//src/testing/catapult_converter:converter_bin",
    ]

    # TODO(https://fxbug.dev/48350): Enable host build for non-linux hosts as well.
    if (host_os == "linux") {
      sources += [ "$root_out_dir/iperf3" ]
      deps += [ "//third_party/iperf:bin" ]
    }
  }
}

_python_benchmarks += [
  "//src/connectivity/network/drivers/network-device/device:benchmarks",
  "//src/connectivity/network/netstack3:benchmarks",
  "//src/connectivity/network/tests/benchmarks",
  "//src/devices/bin/driver_runtime/microbenchmarks",
  "//src/diagnostics:benchmarks",
  "//src/lib/diagnostics:benchmarks",
  "//src/media/audio/audio_core/shared/mixer/tools:benchmarks",
  "//src/starnix:benchmarks",
  "//src/storage/benchmarks/fuchsia:benchmarks",
  "//src/tests/benchmarks",
  "//src/tests/microbenchmarks:benchmarks",
]

group("test") {
  testonly = true

  deps = _python_benchmarks

  deps += [
    "//third_party/sbase:sbase-pkgs",

    # Many tests use sl4f to communicate with the target.
    "//src/testing/sl4f",

    # These are used by netstack benchmarks.
    "//third_party/iperf:iperf3_pkg",

    # These are used by flatland_benchmarks and input_latency_benchmarks
    "//src/ui/examples:flatland-examples",
  ]

  if (!is_asan) {
    deps += [
      # This is used by storage_starnix_benchmarks_test.
      "//src/storage/benchmarks/starnix:benchmarks",
    ]
  }

  foreach(target_name, test_list) {
    deps += [ ":$target_name($host_toolchain)" ]
  }
}

group("tests_to_run_on_internal_builders") {
  testonly = true

  deps = [
    "//src/media/audio/audio_core/shared/mixer/tools:benchmarks",
    "//src/storage/benchmarks/fuchsia:benchmarks",
    "//src/tests/benchmarks:kernel_boot_benchmarks",
  ]
}
